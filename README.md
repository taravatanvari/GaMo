
# GaMo: Gaze and Motion Dataset for Enhanced 3D Pose Estimation

![PoseFusionNet Overview](png/overview.png)
*Figure: An overview of the PoseFusionNet model architecture.*

## Repository Status
ğŸš§ This repository is currently under construction. Stay tuned for updates! ğŸ˜Š

---

## About
This repository accompanies the paper **"Where Does Gaze Lead? Integrating Gaze and Motion for Enhanced 3D Pose Estimation"**, which introduces the GaMo dataset and the PoseFusionNet model. The GaMo dataset integrates gaze data and human joint motion capture data, providing a comprehensive resource for 3D pose estimation, human-object interaction, and human-human interaction studies. ğŸ“Š

This repository hosts:
- ğŸ§  The PoseFusionNet implementation.
- ğŸ› ï¸ Scripts for preprocessing and analyzing motion and gaze data.
- ğŸ¥ Videos of online and offline visualizations.

ğŸ“„ **[Read the Paper](abcdefg)**  
ğŸ“‚ **[Download the Dataset](https://osf.io/jx54y/)**  

---

## Paper Abstract
Understanding and estimating body pose is becoming increasingly important for enhancing user experiences in Virtual Reality (VR). Eye gaze plays a critical role in many VR and Augmented Reality (AR) applications. In this work, we present GaMo, a novel dataset that integrates gaze data and human joint motion capture data, and introduce PoseFusionNet, a model that combines LSTM and Transformer architectures for enhanced 3D pose estimation. The dataset and model provide a foundation for modeling interactions between users and their surroundings with high accuracy.

---

## Dataset
The GaMo dataset contains:
- *154,212 frames* across *83 motion clips*.
-  Full-body motion capture data (41 markers per participant) recorded at 120 FPS.
- Gaze data collected using a *Tobii Eye Tracker II* ğŸ‘€ synchronized with motion capture. 
-  A variety of tasks, including human-object and human-human interactions.

### Key Features:
- Diverse motion scenarios: walking, running, squatting, object manipulation, and more.
---

## Getting Started
### Requirements
- ğŸ Python 3.8+
- ğŸ”¥ PyTorch
- ğŸ“‹ Additional requirements will be provided upon repository completion.

### Installation
âš™ï¸ Instructions for setup and data usage will be added soon.

---

## Usage
- ğŸ“‚ **Dataset**: Follow the dataset link above to download and use the data.
- ğŸ–¥ï¸ **Code**: Scripts for training PoseFusionNet and analyzing results will be available.
- ğŸ› ï¸ **Preprocessing**: Guidelines for synchronizing and processing motion and gaze data.

---

## License
- ğŸ“œ The code in this repository is licensed under the [MIT License](https://github.com/taravatanvari/GaMo?tab=MIT-1-ov-file).
- ğŸ“‚ The GaMo dataset is licensed under the **Creative Commons Attribution 4.0 International (CC BY 4.0)** license. For details, see the [Creative Commons license page](https://creativecommons.org/licenses/by/4.0/).

---

## Citation
If you find this useful, please cite our paper :)

